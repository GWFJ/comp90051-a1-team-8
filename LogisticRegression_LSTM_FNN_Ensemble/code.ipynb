{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOebF5NQ_E8s"
      },
      "source": [
        "# **COMP90051 Assignment 1 2023**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils import data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srcbALHgoZ1N"
      },
      "source": [
        "### Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h0vHBj8dqx3",
        "outputId": "1cadcd2e-4641-4a73-b7a0-9ad68dde1639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_data(path):\n",
        "  with open(path) as file:\n",
        "    data = [json.loads(row) for row in file]\n",
        "  file.close()\n",
        "  return data\n",
        "def l_df(data):\n",
        "  return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "#takes in a df object and builds a tensor matrix of width = 5000 i.e the number of unique words\n",
        "#returns the tensor matrix\n",
        "def build_freq_dataset_bow(data_src,col_name):\n",
        "    freq_dataset = torch.zeros([data_src.shape[0], 5000])\n",
        "    i = 0 #keeps track of the current row\n",
        "    for row_text in data_src[col_name]:\n",
        "        for row_text_index in row_text:\n",
        "            freq_dataset[i][row_text_index] += 1\n",
        "        i += 1\n",
        "    return freq_dataset\n",
        "\n",
        "#takes in a df\n",
        "def build_padded_dataset(data_src,col_name,max_col):\n",
        "    d = data_src.copy(deep=True)\n",
        "    # d = data_src.iloc[0:0]\n",
        "    #df.drop(df[df['Fee'] >= 24000].index, inplace = True)\n",
        "    # print(d.info())\n",
        "    dataset = torch.zeros([d.shape[0], max_col])\n",
        "\n",
        "    row_ind = 0\n",
        "    for i in d[col_name]:\n",
        "        diff = max_col - len(i)\n",
        "\n",
        "        # if(len(i) == 0): #ignore samples with 0 length\n",
        "        #   continue\n",
        "\n",
        "        for u in range(len(i)):\n",
        "            #d[u] = data_src[u] + 1\n",
        "            i[u] += 1\n",
        "\n",
        "        for u in range(diff):\n",
        "            i.append(0)\n",
        "\n",
        "        dataset[row_ind] = torch.Tensor(i)\n",
        "        row_ind += 1\n",
        "    # del d\n",
        "    return dataset\n",
        "\n",
        "#extracts the labels from the source df and returns a tensor.\n",
        "#this is used to build the tensor data loader\n",
        "def extract_output_tensor(data_src,col_name):\n",
        "    response = torch.zeros([data_src.shape[0], 1])\n",
        "    i = 0\n",
        "    for row_response in data_src[col_name]:\n",
        "        response[i] = row_response\n",
        "        i+=1\n",
        "    return response\n",
        "\n",
        "def extract_output_tensor_multi(data_src,ai_col_name,model_name):\n",
        "    response = torch.zeros([data_src.shape[0], 1])\n",
        "    model = torch.zeros([data_src.shape[0], 1])\n",
        "    #extract response\n",
        "    i = 0\n",
        "    for row_response in data_src[ai_col_name]:\n",
        "        response[i] = row_response\n",
        "        i+=1\n",
        "\n",
        "    #extract model\n",
        "    i = 0\n",
        "    for row_model in data_src[model_name]:\n",
        "        model[i] = row_model\n",
        "        i+=1\n",
        "    return response, model\n",
        "\n",
        "#takes in a df data_src of 2 columns for X and output Y and returns a tensor data loader object\n",
        "#a tensor breaks down a large dataset into smaller chunks for lesser memory usage\n",
        "#default batch size is 128\n",
        "def get_torch_data_loader(data_src,data_col_name,label_col_name,is_test,max_col,batch_size=20, shuffle=False):\n",
        "    x = build_padded_dataset(data_src,data_col_name,max_col)\n",
        "    #x = build_x(data_src,data_col_name)\n",
        "    if is_test:\n",
        "        ds = TensorDataset(x) # just load x for test\n",
        "        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle) # create your dataloader\n",
        "    else:\n",
        "        y = extract_output_tensor(data_src,label_col_name)\n",
        "        ds = TensorDataset(x,y)\n",
        "        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle) # create your dataloader\n",
        "\n",
        "def build_tensor_dataset(data_src,data_col_name,label_col_name,is_test,data_format,max_col):\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    x = torch.empty(0,0)\n",
        "\n",
        "    if data_format == \"bow\":\n",
        "        x = build_freq_dataset_bow(data_src,data_col_name)\n",
        "    elif data_format == \"padding\":\n",
        "        x = build_padded_dataset(data_src,data_col_name,max_col) #pad data according to max_col\n",
        "\n",
        "    if is_test:\n",
        "        return TensorDataset(x)\n",
        "    else:\n",
        "        y = extract_output_tensor(data_src,label_col_name)\n",
        "        return TensorDataset(x,y)\n",
        "\n",
        "def build_tensor_dataset_domain2(data_src,data_col_name,label_col_name,is_test,data_format,max_col):\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    x = torch.empty(0,0)\n",
        "\n",
        "    if data_format == \"bow\":\n",
        "        x = build_freq_dataset_bow(data_src,data_col_name)\n",
        "    elif data_format == \"padding\":\n",
        "        x = build_padded_dataset(data_src,data_col_name,max_col) #pad data according to max_col\n",
        "\n",
        "    if is_test:\n",
        "        return TensorDataset(x)\n",
        "    else:\n",
        "        y, model = extract_output_tensor_multi(data_src,label_col_name,\"model\")\n",
        "        model = torch.nan_to_num(model, nan=7.0) #set human generated model value to 7\n",
        "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(model)\n",
        "        model = ohe.transform(model)\n",
        "        model = torch.tensor(model)\n",
        "        # print(model.shape)\n",
        "        # print(x.shape)\n",
        "        # print(y.shape)\n",
        "        # y = extract_output_tensor(data_src,label_col_name)\n",
        "\n",
        "        return TensorDataset(x,y,model)\n",
        "\n",
        "def df_to_tensor(data_src,col_name):\n",
        "    d = data_src.copy()\n",
        "    t = torch.zeros(0,0)\n",
        "\n",
        "    row_ind = 0\n",
        "    #loop rows\n",
        "    for i in d[col_name]:\n",
        "\n",
        "        t[row_ind] = torch.Tensor(i)\n",
        "        row_ind += 1\n",
        "\n",
        "    return t\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ZoigemAKpB"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YHT9616uoZ1P"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cv_test_size = 0.2\n",
        "#Train Data\n",
        "\n",
        "domain1_train = load_data('Holdout Data/domain1_train.json')\n",
        "domain2_train = load_data('Holdout Data/domain2_train.json')\n",
        "hold_out = load_data('Holdout Data/holdout.json')\n",
        "hold_out2 = load_data('Holdout Data/holdout 2.json')\n",
        "#Test Data\n",
        "test_set = load_data('test_set.json')\n",
        "\n",
        "#Dataframes\n",
        "domain1_df = l_df(domain1_train)\n",
        "domain2_df = l_df(domain2_train)\n",
        "domain2_multi_df = l_df(domain2_train)\n",
        "holdout_df = l_df(hold_out)\n",
        "holdout2_df = l_df(hold_out2)\n",
        "test_df = l_df(test_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS65HqMXDFEP",
        "outputId": "a1e170ab-d6c6-4d2f-dd61-ddc72943bf12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13900, 3)\n"
          ]
        }
      ],
      "source": [
        "#Create holdout set (Run once)\n",
        "domain2_train = load_data('./Holdout Data/domain2_train.json')\n",
        "domain2_df = l_df(domain2_train)\n",
        "\n",
        "zero = domain2_df[domain2_df[\"model\"].isin([0])].sample(n=71, replace=False)\n",
        "one = domain2_df[domain2_df[\"model\"].isin([1])].sample(n=71, replace=False)\n",
        "two = domain2_df[domain2_df[\"model\"].isin([2])].sample(n=71, replace=False)\n",
        "three = domain2_df[domain2_df[\"model\"].isin([3])].sample(n=72, replace=False)\n",
        "four = domain2_df[domain2_df[\"model\"].isin([4])].sample(n=71, replace=False)\n",
        "five = domain2_df[domain2_df[\"model\"].isin([5])].sample(n=72, replace=False)\n",
        "six = domain2_df[domain2_df[\"model\"].isin([6])].sample(n=72, replace=False)\n",
        "\n",
        "domain2_df = domain2_df.drop(zero.index)\n",
        "domain2_df = domain2_df.drop(one.index)\n",
        "domain2_df = domain2_df.drop(two.index)\n",
        "domain2_df = domain2_df.drop(three.index)\n",
        "domain2_df = domain2_df.drop(four.index)\n",
        "domain2_df = domain2_df.drop(five.index)\n",
        "domain2_df = domain2_df.drop(six.index)\n",
        "\n",
        "holdout_df = pd.concat([zero,one,two,three,four,five,six])\n",
        "\n",
        "domain2_df.to_json('./Holdout Data/domain2_train_afterholdout.json',orient='records',lines=True)\n",
        "holdout_df.to_json('./Holdout Data/out.json',orient='records',lines=True)\n",
        "\n",
        "print(domain2_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CREATE data loader for domain 2 - using a sampled approach\n",
        "domain2_train = load_data('Holdout Data/domain2_train.json')\n",
        "domain2_df = l_df(domain2_train)\n",
        "\n",
        "#target 0:machine generated 1:human generated\n",
        "max_col = 1075\n",
        "numDataPoints = domain2_df.shape[0]\n",
        "bs = 4000\n",
        "\n",
        "minority_count = domain2_df[domain2_df[\"label\"] == 1].shape[0] #count for human generated\n",
        "majority_count = domain2_df[domain2_df[\"label\"] == 0].shape[0] #count for hAIuman generated\n",
        "\n",
        "x = build_padded_dataset(domain2_df,\"text\",max_col) #pad data according to max_col\n",
        "y = extract_output_tensor(domain2_df,\"label\")\n",
        "\n",
        "y = y.squeeze()\n",
        "y = y.numpy()\n",
        "y = y.astype(int)\n",
        "\n",
        "class_sample_count = np.array([majority_count,minority_count])\n",
        "\n",
        "weight = 1. / class_sample_count\n",
        "\n",
        "\n",
        "# #for target value 0, replace with 1/majority_value, likewise for target value 1 and minority_value\n",
        "# 1 -> 0\n",
        "# 0 > 1\n",
        "samples_weight = np.array([weight[t] for t in y])\n",
        "samples_weight\n",
        "\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "samples_weight = samples_weight.double()\n",
        "\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, 18500)\n",
        "\n",
        "y = torch.from_numpy(y).long()\n",
        "y = torch.reshape(y, (len(y), 1))\n",
        "\n",
        "domain2_sampled_ds = torch.utils.data.TensorDataset(x, y)\n",
        "\n",
        "train_loader = DataLoader(domain2_sampled_ds, batch_size=bs, num_workers=1, sampler=sampler)\n",
        "\n",
        "for i, (data, target) in enumerate(train_loader):\n",
        "    print(\"batch index {}, 0/1: {}/{}\".format(\n",
        "        i,\n",
        "        len(np.where(target.numpy() == 0)[0]),\n",
        "        len(np.where(target.numpy() == 1)[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NUi7PyjoZ1P"
      },
      "source": [
        "#### TensorDatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ry3C9-I-oZ1Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_col = 1075 #maxmi\n",
        "#get a complete dataset of domain1 and domain2\n",
        "# all_data_df = pd.concat([domain1_df, domain2_df]).sample(frac=1).reset_index(drop=True)\n",
        "# all_data_ds = build_tensor_dataset(all_data_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "\n",
        "#domain only datasets\n",
        "# domain1_ds = build_tensor_dataset(domain1_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "domain1_ds = build_tensor_dataset(domain1_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # print(domain2_df.head())\n",
        "# domain2_ds = build_tensor_dataset(domain2_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "\n",
        "# # print(domain2_df.head())\n",
        "# # domain2_1_ds = build_tensor_dataset(model1_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # domain2_2_ds = build_tensor_dataset(model2_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # domain2_3_ds = build_tensor_dataset(model3_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # domain2_4_ds = build_tensor_dataset(model4_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # domain2_5_ds = build_tensor_dataset(model5_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # domain2_6_ds = build_tensor_dataset(model6_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# # domain2_7_ds = build_tensor_dataset(model7_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "\n",
        "holdout_ds = build_tensor_dataset(holdout_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "holdout2_ds = build_tensor_dataset(holdout2_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "#domain 2 dataset with 2150 human generated and 2150 machine (equally distributed among the 7 models)\n",
        "# domain2_equal_parts_ds = build_tensor_dataset(domain2_AI_generated_balanced_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "# alldomain1_domain2equal_ds = build_tensor_dataset(alldomain1_domain2equal_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "test_data = build_padded_dataset(test_df,\"text\",max_col)\n",
        "domain2__multi_ds = build_tensor_dataset_domain2(domain2_multi_df,\"text\",\"label\",False,\"padding\",max_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW93U_9coZ1Q"
      },
      "source": [
        "### Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P5XVRlAVoZ1Q"
      },
      "outputs": [],
      "source": [
        "from numpy.core.numeric import outer\n",
        "#input two tensors, returns a percent value of acurrate predictions\n",
        "def binary_classification_accuracy(predicted,actual):\n",
        "\n",
        "    val = torch.eq(predicted.round().detach(), actual).float()\n",
        "    correct_count = (val == True).sum(dim=0).item()\n",
        "    incorrect_count = actual.size(0)- correct_count\n",
        "    incorrect_count\n",
        "\n",
        "    accuracy = 100 * (correct_count/actual.size(0))\n",
        "    return accuracy\n",
        "\n",
        "#one-off predictions for the test dataset\n",
        "#assumes model is moved back to cpu first\n",
        "def test(model, new_data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(new_data)  # Compute scores\n",
        "        model.train()\n",
        "        return out\n",
        "\n",
        "#test using the validation set\n",
        "def validate(model, criterion, test_loader,device):\n",
        "    test_loss = 0.\n",
        "    test_preds, test_labels = list(), list()\n",
        "    for i, data in enumerate(test_loader):\n",
        "        x, labels = data\n",
        "        x, labels = x.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "          out = model(x)  # Compute scores\n",
        "          test_loss += criterion(out, labels).item()\n",
        "          test_preds.append(out)\n",
        "          test_labels.append(labels)\n",
        "\n",
        "    test_preds = torch.cat(test_preds)\n",
        "    test_labels = torch.cat(test_labels)\n",
        "\n",
        "    test_accuracy = binary_classification_accuracy(test_preds,test_labels)\n",
        "\n",
        "    print('[VALIDATE] Mean loss {:.4f} | Accuracy {:.4f}'.format(test_loss/len(test_loader), test_accuracy))\n",
        "    mean_loss = test_loss/len(test_loader)\n",
        "\n",
        "    return test_accuracy, mean_loss\n",
        "\n",
        "def train_cv(train_dataset,model,optimizer,epochs,k_folds,batch_size,device):\n",
        "    # Initialize the k-fold cross validation\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "    # Loop through each fold\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train_dataset)):\n",
        "        print(f\"Fold {fold + 1}\")\n",
        "        print(\"-------\")\n",
        "\n",
        "        # Define the data loaders for the current fold\n",
        "        train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(train_idx),)\n",
        "        test_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(test_idx),)\n",
        "\n",
        "        # Train the model on the current fold\n",
        "        THRESHOLD = 90\n",
        "        LOG_INTERVAL = 250\n",
        "        MAX_VALIDATION_DECREASE = 50\n",
        "        validation_decrease_count = 50 #keeps track how many times the validation accuracy worsened. Once it reaches max, stop\n",
        "        prev_val_meanloss = 999\n",
        "        validation_accuracy, validation_loss, running_loss, running_accuracy = list(), list(), list(), list()\n",
        "        start_time = time.time()\n",
        "        criterion = torch.nn.BCELoss() #includes the sigmoid function for binary classification\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.\n",
        "            for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
        "\n",
        "                x, labels = data\n",
        "                x, labels = x.to(device), labels.to(device)\n",
        "                out = model(x)\n",
        "                train_acc = binary_classification_accuracy(out,labels)\n",
        "                loss = criterion(out,labels)\n",
        "\n",
        "                loss.backward()               # Backward pass (compute parameter gradients)\n",
        "                optimizer.step()              # Update weight parameter using SGD\n",
        "                optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
        "\n",
        "                # ============================================================================\n",
        "                # You can safely ignore the boilerplate code below - just reports metrics over\n",
        "                # training and test sets\n",
        "\n",
        "                #running_loss.append(loss.item())\n",
        "                running_accuracy.append(train_acc)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if i % LOG_INTERVAL == 0:  # Log training stats\n",
        "                    deltaT = time.time() - start_time\n",
        "                    mean_loss = epoch_loss / (i+1)\n",
        "                    losses.append(mean_loss)\n",
        "                    print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch,\n",
        "                        i, len(train_loader), mean_loss, train_acc, deltaT))\n",
        "\n",
        "            print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
        "\n",
        "            validation_acc, validate_mean_loss = validate(model, criterion, test_loader,device)\n",
        "\n",
        "            #stop if accuracy is greater than a threshold\n",
        "            if validation_acc >= THRESHOLD:\n",
        "              print(\"Required validation accuracy reached\")\n",
        "              break\n",
        "            #stop if mean validation loss decreases consecutively\n",
        "            if validate_mean_loss >= prev_val_meanloss:\n",
        "                validation_decrease_count += 1\n",
        "\n",
        "                if validation_decrease_count == MAX_VALIDATION_DECREASE:\n",
        "                    print(\"Training stopped as validation accuracy is going down\")\n",
        "                    break\n",
        "            else:\n",
        "                validation_decrease_count = 0\n",
        "                prev_val_meanloss = validate_mean_loss\n",
        "\n",
        "            prev_val_accuracy = validation_acc\n",
        "            validation_loss.append(validate_mean_loss)\n",
        "            validation_accuracy.append(validation_acc)\n",
        "\n",
        "\n",
        "        #plot loss graph\n",
        "        plt.plot(losses, linestyle = 'dotted')\n",
        "        plt.title(\"Train Loss\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(validation_accuracy, linestyle = 'dotted')\n",
        "        plt.title(\"Validation Accuracy\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(validation_loss, linestyle = 'dotted')\n",
        "        plt.title(\"Validation Loss\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ygrH3XKJJuT8"
      },
      "outputs": [],
      "source": [
        "def validate_multi_output(model, criterion, criterion2, test_loader,device):\n",
        "    test_loss = 0.\n",
        "    test_preds, test_labels, model_loss = list(), list(), list()\n",
        "    for i, data in enumerate(test_loader):\n",
        "        x, labels,models = data\n",
        "        x, labels, models = x.to(device), labels.to(device),models.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          out,pred_model = model(x)  # Compute scores\n",
        "          test_loss += criterion(out, labels)\n",
        "          test_preds.append(out)\n",
        "          test_labels.append(labels)\n",
        "\n",
        "    test_preds = torch.cat(test_preds)\n",
        "    test_labels = torch.cat(test_labels)\n",
        "\n",
        "    test_accuracy = binary_classification_accuracy(test_preds,test_labels)\n",
        "\n",
        "    print('[VALIDATE] Mean loss {:.4f} | Accuracy {:.4f} '.format(test_loss/len(test_loader), test_accuracy))\n",
        "    mean_loss = test_loss/len(test_loader)\n",
        "    return test_accuracy, mean_loss\n",
        "\n",
        "def train_cv_multi_output(train_dataset,model,optimizer,epochs,k_folds,batch_size,device):\n",
        "    # Initialize the k-fold cross validation\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "    # Loop through each fold\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train_dataset)):\n",
        "        print(f\"Fold {fold + 1}\")\n",
        "        print(\"-------\")\n",
        "\n",
        "        # Define the data loaders for the current fold\n",
        "        train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(train_idx),)\n",
        "        test_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(test_idx),)\n",
        "        \n",
        "        # Train the model on the current fold\n",
        "        THRESHOLD = 95\n",
        "        LOG_INTERVAL = 250\n",
        "        MAX_VALIDATION_DECREASE = 30\n",
        "        validation_decrease_count = 10 #keeps track how many times the validation accuracy worsened. Once it reaches max, stop\n",
        "        prev_val_meanloss = 999\n",
        "\n",
        "        validation_accuracy, validation_loss, running_loss, running_accuracy = list(), list(), list(), list()\n",
        "        start_time = time.time()\n",
        "\n",
        "        criterion = torch.nn.BCELoss() #includes the sigmoid function for binary classification\n",
        "        criterion_model = torch.nn.CrossEntropyLoss()\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.\n",
        "            for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
        "\n",
        "                x, labels,models = data\n",
        "                x, labels,models = x.to(device), labels.to(device), models.to(device)\n",
        "                out, pred_models = model(x) #automatically calls the forward function of the model\n",
        "\n",
        "                train_acc = binary_classification_accuracy(out,labels)\n",
        "                loss = criterion(out,labels)\n",
        "                model_loss = criterion_model(pred_models,models)\n",
        "\n",
        "                loss = loss + (model_loss*5)\n",
        "                loss.backward()               # Backward pass (compute parameter gradients)\n",
        "                optimizer.step()              # Update weight parameter using SGD\n",
        "                optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
        "\n",
        "                # ============================================================================\n",
        "                # You can safely ignore the boilerplate code below - just reports metrics over\n",
        "                # training and test sets\n",
        "                running_accuracy.append(train_acc)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if i % LOG_INTERVAL == 0:  # Log training stats\n",
        "                    deltaT = time.time() - start_time\n",
        "                    mean_loss = epoch_loss / (i+1)\n",
        "                    losses.append(mean_loss)\n",
        "                    print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch,\n",
        "                        i, len(train_loader), mean_loss, train_acc, deltaT))\n",
        "\n",
        "            print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
        "\n",
        "            validation_acc, validate_mean_loss = validate_multi_output(model, criterion, criterion_model, test_loader,device)\n",
        "\n",
        "            #stop if accuracy is greater than a threshold\n",
        "            if validation_acc >= THRESHOLD:\n",
        "              print(\"Required validation accuracy reached\")\n",
        "              break\n",
        "            #stop if mean validation loss decreases consecutively\n",
        "            if validate_mean_loss >= prev_val_meanloss:\n",
        "                validation_decrease_count += 1\n",
        "\n",
        "                if validation_decrease_count == MAX_VALIDATION_DECREASE:\n",
        "                    print(\"Training stopped as validation accuracy is going down\")\n",
        "                    break\n",
        "            else:\n",
        "                validation_decrease_count = 0\n",
        "                prev_val_meanloss = validate_mean_loss\n",
        "\n",
        "            prev_val_accuracy = validation_acc\n",
        "            validation_loss.append(validate_mean_loss)\n",
        "            validation_accuracy.append(validation_acc)\n",
        "\n",
        "\n",
        "        #plot loss graph\n",
        "        plt.plot(losses, linestyle = 'dotted')\n",
        "        plt.title(\"Train Loss\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(validation_accuracy, linestyle = 'dotted')\n",
        "        plt.title(\"Validation Accuracy\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(validation_loss, linestyle = 'dotted')\n",
        "        plt.title(\"Validation Loss\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqW0IcCooZ1Q"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "_uChQJrgoZ1Q"
      },
      "outputs": [],
      "source": [
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, input_dim,out_dim,vocab_size,embedding_vector_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_vector_size)\n",
        "        self.dropout = nn.Dropout(0.5) #dropout to avoid overfitting\n",
        "        self.layer1 = nn.Linear(input_dim, input_dim)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(input_dim, input_dim)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(input_dim, input_dim)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.output = nn.Linear(input_dim, out_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x).to(torch.int64)\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x) #remove unnecessary neurons\n",
        "        #reshape x after dropout\n",
        "        bs, _, _ = x.shape\n",
        "        x = torch.nn.functional.adaptive_avg_pool1d(x, 1).reshape(bs, -1)\n",
        "        x = self.act1(self.layer1(x))\n",
        "        x = self.act2(self.layer2(x))\n",
        "        x = self.act3(self.layer3(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, input_sequence):\n",
        "        # apply GRU to full input sequence, and retain final hidden state\n",
        "        _, hidden = self.gru(input_sequence)\n",
        "        # couple final hidden state to multiclass classifier, i.e., softmax output\n",
        "        output = self.h2o(hidden.view(1, -1))\n",
        "        output = self.sigmoid(output)\n",
        "        # output = F.log_softmax(output, dim=1)\n",
        "        return output\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_dim,vocab_size, hidden_size, num_layers, bidirectional,dropout):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size + 1,embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            bidirectional = bidirectional,\n",
        "                            dropout = dropout,\n",
        "                            batch_first = True)\n",
        "        # Dense layer to predict\n",
        "        self.fc = nn.Linear(hidden_size * 2,1)\n",
        "        # Prediction activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x).to(torch.int64)\n",
        "        embedded = self.embedding(x)\n",
        "        packed_output,(hidden_state,cell_state) = self.lstm(embedded)\n",
        "\n",
        "        # Concatenating the final forward and backward hidden states\n",
        "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
        "\n",
        "        dense_outputs=self.fc(hidden)\n",
        "\n",
        "        #Final activation function\n",
        "        print(dense_outputs)\n",
        "        outputs=self.sigmoid(dense_outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "#outputs two values, the model and the binary 0 or 1\n",
        "class LSTMModel_MultiObjective(nn.Module):\n",
        "    def __init__(self, embedding_dim,vocab_size, hidden_size, num_layers, bidirectional,dropout):\n",
        "        super(LSTMModel_MultiObjective, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size + 1,embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            bidirectional = bidirectional,\n",
        "                            dropout = dropout,\n",
        "                            batch_first = True)\n",
        "        # Dense layer to predict\n",
        "        self.fc = nn.Linear(hidden_size * 2,1)\n",
        "        self.fc_models = nn.Linear(hidden_size * 2,8) #8 classes of models - 8 outputs\n",
        "        # Prediction activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x).to(torch.int64)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        packed_output,(hidden_state,cell_state) = self.lstm(embedded)\n",
        "\n",
        "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
        "\n",
        "        dense_outputs=self.fc(hidden)\n",
        "        model_out=self.fc_models(hidden)\n",
        "\n",
        "        human_ai_output =self.sigmoid(dense_outputs)\n",
        "        return human_ai_output, model_out\n",
        "\n",
        "class LogisticRegressionModel(torch.nn.Module):\n",
        "     def __init__(self, input_dim, output_dim):\n",
        "         super(LogisticRegressionModel, self).__init__()\n",
        "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "     def forward(self, x):\n",
        "         outputs = torch.sigmoid(self.linear(x))\n",
        "        #  print(outputs)\n",
        "         return outputs\n",
        "\n",
        "# Neural Network should have a sigmoid activation function if you are using BCELoss()\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weight\n",
        "\n",
        "    def forward(self,x):\n",
        "        # print(x.size())\n",
        "        out = self.fc1(x)\n",
        "        # out = self.relu(out)\n",
        "        out = torch.sigmoid(self.fc2(out)) #sigmoid as we use BCELoss\n",
        "        # print(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "bzIq10rgoZ1R",
        "outputId": "4ca60f74-8453-44cc-8e37-3508af0d343b"
      },
      "outputs": [],
      "source": [
        "#Test the DeepNN framework\n",
        "#Test the DeepNN framework\n",
        "k_folds = 5\n",
        "batch_size = 100 #1 for gru\n",
        "num_epochs = 50\n",
        "max_col = 1075 #found from checking max length of all df domain1, domain2 and test_df e.g domain2_df[\"text\"].str.len().max()\n",
        "embedding_vector_size = 300 #each token will have a 128 vector in the embedding layer\n",
        "vocab_size = 5000\n",
        "num_features = max_col\n",
        "hidden_size = 64 #32 used for domain2\n",
        "BIDIRECTION = True\n",
        "DROPOUT = 0.2\n",
        "NUM_LAYERS =  1\n",
        "\n",
        "#set to the length of the longest sample\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## ******* Logistic Regression Model\n",
        "# model = LogisticRegressionModel(max_col,1)\n",
        "# model.to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "# # train_cv(domain2_equal_parts_ds,model,optimizer,num_epochs,k_folds,batch_size)\n",
        "# train_cv(domain2_1_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "\n",
        "# ## ******* Logistic Regression Model\n",
        "# model = LogisticRegressionModel(max_col,1)\n",
        "# model.to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "# # train_cv(domain2_equal_parts_ds,model,optimizer,num_epochs,k_folds,batch_size)\n",
        "# train_cv(domain1_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "\n",
        "## ******* GRU Model\n",
        "# model = GRUModel(num_features, hidden_size, 1)\n",
        "# model.to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "# #optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# train_cv(domain1_ds,model,optimizer,num_epochs,k_folds,batch_size)\n",
        "\n",
        "## ******* LSTM Model - domain 1\n",
        "\n",
        "# domain2_train = load_data('Holdout Data/domain2_train.json')\n",
        "# domain2_df = l_df(domain2_train)\n",
        "# domain2_1_ds = build_tensor_dataset(model1_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "\n",
        "#model = LSTMModel(embedding_vector_size,vocab_size, hidden_size, NUM_LAYERS,BIDIRECTION,DROPOUT)\n",
        "# model = torch.load('Output/checkpoint2_model.pt')#\n",
        "# optimizer =  torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "# model.to(device)\n",
        "# train_cv(holdout2_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "# domain2_ds\n",
        "# domain2_equal_parts_ds\n",
        "# domain2_3_ds\n",
        "\n",
        "# ******* LSTM Model - multiobjective\n",
        "\n",
        "# domain2_train = load_data('Holdout Data/domain2_train.json')\n",
        "# domain2_df = l_df(domain2_train)\n",
        "# domain2_1_ds = build_tensor_dataset(model1_df,\"text\",\"label\",False,\"padding\",max_col)\n",
        "\n",
        "model = LSTMModel_MultiObjective(embedding_vector_size,vocab_size, hidden_size, NUM_LAYERS,BIDIRECTION,DROPOUT)\n",
        "# model = torch.load('Output/checkpoint2_model.pt')#\n",
        "optimizer =  torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "model.to(device)\n",
        "train_cv_multi_output(domain2__multi_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "# domain2_ds\n",
        "# domain2_equal_parts_ds\n",
        "# domain2_3_ds\n",
        "\n",
        "## ******* LSTM Model - domain 2\n",
        "\n",
        "# num_epochs = 200\n",
        "\n",
        "# #model = torch.load('Output/checkpoint1_domain1.pt')#\n",
        "# model = LSTMModel(embedding_vector_size,vocab_size, hidden_size, NUM_LAYERS,BIDIRECTION,DROPOUT)\n",
        "# model.to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "# # num_epochs = 200\n",
        "# # batch_size = 5\n",
        "\n",
        "# # model = LogisticRegressionModel(max_col,1)\n",
        "# # model.to(device)\n",
        "# # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# # # train_cv(domain2_equal_parts_ds,model,optimizer,num_epochs,k_folds,batch_size)\n",
        "# # train_cv(domain1_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "\n",
        "\n",
        "# # Randomly sample datasets from domain2 and pass for model training\n",
        "# #reduce the num epochs as training will be for many datasets\n",
        "# for i, (data, target) in enumerate(train_loader):\n",
        "#     print(\"batch index {}, 0/1: {}/{}\".format(\n",
        "#         i,\n",
        "#         len(np.where(target.numpy() == 0)[0]),\n",
        "#         len(np.where(target.numpy() == 1)[0])))\n",
        "#     new_ds = TensorDataset(data.float(),target.float())\n",
        "#     train_cv(new_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "\n",
        "#     del new_ds\n",
        "\n",
        "## ********* LSTM Stacking Ensemble\n",
        "\n",
        "# num_epochs = 5\n",
        "# batch_size = 4\n",
        "\n",
        "# model1 = torch.load('Output/domain1_model_fixedpadding.pt')\n",
        "# model2 = torch.load('Output/domain2_model_fixedpadding.pt')\n",
        "# model1.to(device)\n",
        "# model2.to(device)\n",
        "\n",
        "# for param in model1.parameters():\n",
        "#     param.requires_grad_(False)\n",
        "\n",
        "# for param in model2.parameters():\n",
        "#     param.requires_grad_(False)\n",
        "\n",
        "# model1.eval()\n",
        "# model2.eval()\n",
        "\n",
        "# embedding_vector_size = 128 #each token will have a 128 vector in the embedding layer\n",
        "\n",
        "# model = torch.load('Output/meta_model_3_758.pt')#\n",
        "# # model = EnsembleModel_2Models(model1,model2,embedding_vector_size * 2,1,hidden_size)\n",
        "# # model = EnsembleModel_2Models_LogisticRegression(model1,model2)\n",
        "\n",
        "# model.to(device)\n",
        "# # for name, param in model.named_parameters():\n",
        "# #     if param.requires_grad:\n",
        "# #         print(name, param.data)\n",
        "# optimizer =  torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "# # ##train over a combined dataset\n",
        "# train_cv(holdout2_ds,model,optimizer,num_epochs,k_folds,batch_size,device)\n",
        "## ******* Deep FNN\n",
        "\n",
        "# # train_cv(ds,model,optimizer,num_epochs,k_folds,batch_size)\n",
        "# domain1_model = DeepNN(num_features, 1,vocab_size,embedding_vector_size)\n",
        "# domain1_model.to(device)\n",
        "# domain1_model_optimizer = optim.Adam(domain1_model.parameters(), lr=0.0001)\n",
        "\n",
        "# train_cv(domain1_ds,domain1_model,domain1_model_optimizer,num_epochs,k_folds,batch_size)\n",
        "\n",
        "# for param in domain1_model.parameters():\n",
        "#     param.requires_grad_(False)\n",
        "\n",
        "\n",
        "#Save entire model to file\n",
        "torch.save(model, 'model_backup.pt')\n",
        "\n",
        "# #Test output\n",
        "model.to(\"cpu\")\n",
        "prediction = test(model,test_data).round()\n",
        "np.savetxt('test_out.txt', prediction.cpu().numpy(),fmt=\"%d\")\n",
        "\n",
        "del model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SGdYc4NTYNXM"
      },
      "outputs": [],
      "source": [
        "class EnsembleModel_2Models_CombineOutput(nn.Module):\n",
        "    def __init__(self, modelA, modelB, input_dim, output_dim):\n",
        "        super(EnsembleModel_2Models_CombineOutput, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "\n",
        "    def forward(self, x):\n",
        "        #make prediction using each model\n",
        "        x1 = self.modelA(x.clone())\n",
        "        x2 = self.modelB(x.clone())\n",
        "\n",
        "        print(\"x1\")\n",
        "        print(x1)\n",
        "        print(\"x2\")\n",
        "        print(x2)\n",
        "        #concatenate outputs horizontally. I.e x will look like [[0 1],[1 1],..,[1 1]]\n",
        "        x = torch.cat((x1, x2), dim=1) #x n * 2 matrix\n",
        "        print(\"xt\")\n",
        "\n",
        "        print(x)\n",
        "        xt = torch.max(x, dim=1)\n",
        "        print(\"xt\")\n",
        "        print(xt)\n",
        "\n",
        "        xt = torch.reshape(xt, (1000,1))\n",
        "        return xt\n",
        "#contains a simple feedforward network to learn how to use the base models to predict new data\n",
        "class EnsembleModel_2Models(nn.Module):\n",
        "    def __init__(self, modelA, modelB, input_dim, output_dim,hidden_size):\n",
        "        super(EnsembleModel_2Models, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        # Remove last linear layer\n",
        "        self.modelA.fc = nn.Identity()\n",
        "        self.modelA.sigmoid = nn.Identity()\n",
        "        self.modelB.fc = nn.Identity()\n",
        "        self.modelB.sigmoid = nn.Identity()\n",
        "        # one hidden layer and an output layer for simple feed-forward network\n",
        "        self.hidden = nn.Linear(hidden_size * 4, 150) #hidden nuurons = 2/3 of input\n",
        "        self.hidden_act = nn.ReLU()\n",
        "        self.hidden2 = nn.Linear(150, 150) #hidden nuurons = 2/3 of input\n",
        "        self.hidden2_act = nn.ReLU()\n",
        "        self.hidden3 = nn.Linear(150, 70) #hidden nuurons = 2/3 of input\n",
        "        self.hidden3_act = nn.ReLU()\n",
        "        self.output = nn.Linear(70, 1)\n",
        "        self.output_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #make prediction using each model\n",
        "        x1 = self.modelA(x.clone())\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = self.modelB(x.clone())\n",
        "        x2 = x2.view(x2.size(0), -1)\n",
        "        x = torch.cat((x1, x2), dim=1) #x n * 2 matrix\n",
        "        outputs = self.hidden(x)\n",
        "        outputs = self.hidden_act(outputs)\n",
        "        outputs = self.hidden2(outputs)\n",
        "        outputs = self.hidden2_act(outputs)\n",
        "        outputs = self.hidden3(outputs)\n",
        "        outputs = self.hidden3_act(outputs)\n",
        "        outputs = self.output(outputs)\n",
        "        outputs = self.output_act(outputs)\n",
        "        return outputs\n",
        "\n",
        "class EnsembleModel_2Models_LogisticRegression(nn.Module):\n",
        "    def __init__(self, modelA, modelB):\n",
        "        super(EnsembleModel_2Models_LogisticRegression, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        # Remove last linear layer\n",
        "        self.modelA.fc = nn.Identity()\n",
        "        self.modelA.sigmoid = nn.Identity()\n",
        "        self.modelB.fc = nn.Identity()\n",
        "        self.modelB.sigmoid = nn.Identity()\n",
        "        self.output = nn.Linear(hidden_size * 4, 1)\n",
        "        self.output_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #make prediction using each model\n",
        "        x1 = self.modelA(x.clone())\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = self.modelB(x.clone())\n",
        "        x2 = x2.view(x2.size(0), -1)\n",
        "\n",
        "        x = torch.cat((x1, x2), dim=1) #concat probabilities\n",
        "        outputs = self.output(x)\n",
        "        outputs = self.output_act(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class EnsembleModel_3Models(nn.Module):\n",
        "    def __init__(self, modelA, modelB, modelC, input_dim, output_dim):\n",
        "        super(EnsembleModel_3Models, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.modelC = modelC\n",
        "        # Remove last linear layer\n",
        "        self.modelA.sigmoid = nn.Identity()\n",
        "        self.modelB.sigmoid = nn.Identity()\n",
        "        self.modelC.sigmoid = nn.Identity()\n",
        "        # Create new classifier\n",
        "        self.classifier = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #make prediction using each model\n",
        "        x1 = self.modelA(x.clone())\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = self.modelB(x.clone())\n",
        "        x2 = x2.view(x2.size(0), -1)\n",
        "        x3 = self.modelC(x.clone())\n",
        "        x3 = x3.view(x3.size(0), -1)\n",
        "\n",
        "        #concatenate outputs horizontally. I.e x will look like [[0 1],[1 1],..,[1 1]]\n",
        "        x = torch.cat((x1, x2, x3), dim=1) #x n * 2 matrix\n",
        "        xt = torch.add(x1, x2) #sum linear layer outputs\n",
        "        xt = torch.add(xt, x3) #sum linear layer outputs\n",
        "        xt = torch.div(xt, 3) #average out\n",
        "        outputs = torch.sigmoid(xt)\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Load a model from file and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "FxMLlLdN6MCd",
        "outputId": "26d105f3-5c78-40e8-83c4-0301aaa77165"
      },
      "outputs": [],
      "source": [
        "model1 = torch.load('Output/checkpoint3.pt',map_location=torch.device('cpu'))\n",
        "\n",
        "for param in model1.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "model1.eval()\n",
        "embedding_vector_size = 128 #each token will have a 128 vector in the embedding layer\n",
        "\n",
        "prediction = test(model,test_data).round()\n",
        "np.savetxt('Output/test_out.txt', prediction.cpu().numpy(),fmt=\"%d\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "OMx11vI2DFEW",
        "outputId": "133da250-a076-4f33-d059-9f259bfdff5a"
      },
      "outputs": [],
      "source": [
        "# Test against a specific data source\n",
        "\n",
        "max_col = 1075\n",
        "model1 = torch.load('Output/checkpoint2_model.pt',map_location=torch.device('cpu'))\n",
        "\n",
        "for param in model1.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "dataloader = get_torch_data_loader(domain1_df,\"text\",\"label\",False,max_col,batch_size=1000,shuffle=True)\n",
        "\n",
        "for i, data in enumerate(dataloader):  # Loop over elements in training set\n",
        "    x, labels = data\n",
        "\n",
        "    prediction = test(model1,x).round()\n",
        "    test_accuracy = binary_classification_accuracy(prediction,labels)\n",
        "\n",
        "    print('[TEST] Accuracy {:.4f}'.format(test_accuracy))\n",
        "\n",
        "del model1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
